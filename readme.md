# Trabajo PrÃ¡ctico: TinyGPT y Mixture of Experts (MoE)

## ğŸ“š DescripciÃ³n

Este repositorio contiene la implementaciÃ³n de **TinyGPT**, un modelo Transformer de prÃ¡ctica, y su extensiÃ³n a **Mixture of Experts (MoE)**. El objetivo es:

- **Comparar** un GPT denso vs. un MoE en cuanto a:

  - Arquitectura y nÃºmero de parÃ¡metros.
  - Rendimiento en entrenamiento (soft-mixture + aux-loss) e inferencia (hard top-1 routing).
  - MÃ©tricas de calidad: Perplejidad en set de validaciÃ³n.
  - Throughput (tokens/s) y uso de memoria.

- **Experimentar** con tÃ©cnicas de generaciÃ³n de texto: greedy, temperatura, top-k y top-p (nucleus sampling).

## ğŸ—‚ Estructura de Carpetas

```
â”œâ”€â”€ TinyGPT.ipynb           # Notebook principal con todos los pasos
â”œâ”€â”€ trainer.py              # LÃ³gica de entrenamiento y evaluaciÃ³n
â”œâ”€â”€ checkpoints/            # Checkpoints guardados
â”‚   â””â”€â”€ dense/              # Modelo TinyGPT con FFN
â”‚   â””â”€â”€ moe/                # Modelo TinyGPT MoE
â”œâ”€â”€ runs/                   # Checkpoints guardados
â”‚   â””â”€â”€ experiment1/        # Tensorboard para TinyGPT con FFN
â”‚   â””â”€â”€ experiment2/        # Tensorboard para TinyGPT MoE
â””â”€â”€ README.md               # Este archivo
```

## ğŸš€ Uso

### 1. Entrenamiento

```python
writer = SummaryWriter(log_dir="./runs/<...>")

trainer = Trainer(
    model=<...>,
    train_data_loader=train_loader,
    test_data_loader=val_loader,
    loss_fn=loss_fn,
    gradient_accumulation_steps=grad_accum,
    optimizer=optimizer,
    scheduler=scheduler,
    device=device,
    save_dir="./checkpoints/<...>",
    save_every_n=500
)
```

- Guarda checkpoints en `checkpoints/`.
- Registra mÃ©tricas con TensorBoard en `runs/`.

### 2. Inferencia y GeneraciÃ³n

```python
from utils import generateV2, encode, decode
import torch

# Cargar modelo
model = model(config).to(device)
model.load_state_dict(torch.load('models/model_final.pt'))
model.eval()

# Generar texto
output, throughput = generateV2(
    model=model,
    device=device,
    prompt="To be or not to be,",
    max_new_tokens=100,
    temperature=0.7,
    top_k=10,
    top_p=0.9
)
print(output)
print(f"Throughput: {throughput:.1f} tokens/s")
```

### 3. EvaluaciÃ³n de Perplejidad

- Throughput desde la inferencia con generateV2 (GPU y CPU)
- Cantidad de parÃ¡metros
- Perplejidad
    ```python
    from trainer import compute_perplexity
    import torch.nn as nn

    loss_fn = nn.CrossEntropyLoss(reduction='sum')
    dense_ppl = compute_perplexity(dense_model, val_loader, loss_fn, device)
    moe_ppl   = compute_perplexity(moe_model,   val_loader, loss_fn, device)
    print(f"Dense PPL: {dense_ppl:.2f}")
    print(f"MoE   PPL: {moe_ppl:.2f}")
    ```

## ğŸ“ˆ Resultados y Benchmark

- **ParÃ¡metros**: Dense vs MoE (4 expertos Ã— capa FFN)
- **Perplejidad** en validaciÃ³n: `ppl_dense` vs `ppl_moe`
- **Throughput** (tokens/s) en GPU y CPU
- **Curvas** de Train Loss / Val Loss / Learning Rate (ver `notebooks/Graphs.ipynb`)

## ğŸ“‘ Conclusiones

- MoE aumenta la capacidad sin incrementar linealmente la latencia en inferencia *si* el modelo es lo suficientemente grande o el routing estÃ¡ optimizado.
- En toy-models pequeÃ±os, el overhead de routing puede superar la ganancia de ejecutar un solo experto.
- MÃ©tricas de perplejidad y throughput muestran trade-offs claros entre calidad y velocidad.

## ğŸ“– Referencias

## ğŸ“– Referencias

- **Switch Transformer** (Fedus et al., 2021) â€” *Efficient Mixture of Experts* ([https://arxiv.org/abs/2201.00084](https://arxiv.org/abs/2201.00084))
- **ST-MoE** (Zoph et al., 2022) â€” *Scaling MoE Models* ([https://arxiv.org/abs/2202.08906](https://arxiv.org/abs/2202.08906))
- **A Review of Sparse Expert Models in Deep Learning** (Wright et al., 2022) â€” *Sparse Expert Models* ([https://arxiv.org/pdf/2209.01667](https://arxiv.org/pdf/2209.01667))
- **The Curious Case of Neural Text Degeneration** (Holtzman et al., 2019) â€” *The Curious Case of Neural Text Degeneration* ([https://arxiv.org/abs/1904.09751](https://arxiv.org/abs/1904.09751))
- **Closing the Curious Case of Neural Text Degeneration** (Finlayson et al., 2023) â€” *Closing the Curious Case of Neural Text Degeneration* ([https://openreview.net/pdf?id=dONpC9GL1o](https://openreview.net/pdf?id=dONpC9GL1o))
- **Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation** (Xu et al., 2022) â€” *Learning to Break the Loop* ([https://arxiv.org/pdf/2206.02369](https://arxiv.org/pdf/2206.02369))
- **On Decoding Strategies for Neural Text Generators** (Wiher et al., 2022) â€” *On Decoding Strategies for Neural Text Generators* ([https://arxiv.org/pdf/2203.15721](https://arxiv.org/pdf/2203.15721))


## ğŸ“ Licencia

Este cÃ³digo esta basado en la especializaciÃ³n de CEIA de FIUBA, repo de la materia [NLP2](https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/CEIA-LLMIAG/blob/main/ClaseIV/TinyGPT.ipynb) *Author: Abraham R.*

